{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7eOpezzImolE"
   },
   "source": [
    "## Logistics Regression (Using gradient ascent algorithm)\n",
    "**Author**: Evi Ofekeze\n",
    "\n",
    "This notebook implements the gradient ascent algorithm for logistics regression. Note that this is slighty different from the gradient descent algorithm. However, the follwoing code can be adapted for gradient descent by changing the sign. The mathetical formulation for the algorithm can be found in the appendix at the end of the document if you are into that kind of thing. However you can totally ignore the information in the appendix.\n",
    "\n",
    "\n",
    "**Gradient Ascent Algorithm for Logistic Regression**\n",
    "\n",
    "1. $gradw =$ all zeros (to compute the first gradient)\n",
    "2. $gradb = 0$ (to compute the second gradient)\n",
    "3. for *epoch* in $[1,2,...maxiter]$:\n",
    "\t1. for $x^{(i)},y^{(i}$ for $i \\in [i, m]$ in training data:\n",
    "\n",
    "\t\t1. $gradw \\leftarrow gradw$ + $x^{(i)} \\cdot (y^{(i)} - h_\\theta(x^{(i)})$\n",
    "\t\t2. $gradb \\leftarrow gradb + (y^{(i)} - h_\\theta(x^{(i)})$\n",
    "\t\t3. $w \\leftarrow w + \\eta gradw - \\alpha w$\n",
    "\t\t4. $w \\leftarrow w + \\eta gradw$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticsRegression:\n",
    "    \n",
    "    \"\"\"\n",
    "    The Class contains a preedict and fit function for a logistics regression problem\n",
    "    Input:\n",
    "        Featrue matrix\n",
    "        Outcome/Class vector\n",
    "    Output:\n",
    "        Model parameter\n",
    "        Intercept\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.01, max_iteration=1000, c = 0.1):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iteration = max_iteration\n",
    "        self.c = c\n",
    "\n",
    "    def __sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        The logit/sigmoid function for that return value between 0 and 1\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        A fit funtion for the algorithm above\n",
    "        \"\"\"\n",
    "        \n",
    "        self.weights = np.zeros((X.shape[1],1))\n",
    "        self.bias = np.zeros((1,1))\n",
    "        \n",
    "        for time in range(self.max_iteration):\n",
    "            z = np.dot(X, self.weights) + self.bias\n",
    "            h = self.__sigmoid(z)\n",
    "    \n",
    "            gradweight = np.dot(X.T, (y - h))\n",
    "            gradbias = np.sum((y - h))\n",
    "           \n",
    "            self.weights += self.learning_rate * (gradweight - (1/self.c) * self.weights)\n",
    "            self.bias += self.learning_rate * gradbias\n",
    "        \n",
    "        self.coef_ = self.weights\n",
    "        self.intercept_ = self.bias\n",
    "\n",
    "    def predict_prob(self, X):\n",
    "        \"\"\"\n",
    "        A predict function to return probabilities \n",
    "        \"\"\"\n",
    "        return self.__sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "\n",
    "    def predict(self, X): \n",
    "        \"\"\"\n",
    "        A predict funtion to place in their respective classes\n",
    "        \"\"\"\n",
    "        return (self.predict_prob(X) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Implentation**\n",
    "\n",
    "The functinality of the algorithm will be tested using the iris dataset from sklearn library. We shall perform a binary classification by for setosa and others. The result will be compared to scikit learns logistics regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing standard libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Importing Machine Learning Libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data**\n",
    "\n",
    "The data consist of the following:\n",
    "- Outcome/class variable stored in the target.\n",
    "- The class names are stored in the variable target_names.\n",
    "- The predictor variables stored as in the variable data\n",
    "\n",
    "Setosa is represented by as 0's, verisicolor and virginica is coded as 1' and 2's respectively. we will convert none zeros to ones for this binary classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Feature variables\n",
    "X = iris.data\n",
    "\n",
    "# Target variables\n",
    "y = (iris.target != 0) * 1\n",
    "y = y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is customary to set aside some data for testing, this is no exception!\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We standardized the data, to prevent domineering effects of large variable. \n",
    "# The following code ensures a mean of 0 and a standard deviation of 1\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticsRegression(learning_rate=0.01, max_iteration=100000, c = 1)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.04274213],\n",
       "       [-1.12900293],\n",
       "       [ 1.69946447],\n",
       "       [ 1.54545672]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.37787162]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Implentation: Using SkLearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_model = LogisticRegression(max_iter= 100000)\n",
    "sk_model = sk_model.fit(X_train,y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.04274427, -1.12899966,  1.69946559,  1.54545684]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results we see that both methods produced comparable results. It works!!!\n",
    "\n",
    "No Machine Learning model is complete without testing on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metric</th>\n",
       "      <th>Scratch</th>\n",
       "      <th>SkLearn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metric     Scratch  SkLearn\n",
       "Accuracy       1.0      1.0\n",
       "Precision      1.0      1.0\n",
       "Recall         1.0      1.0\n",
       "F1 Score       1.0      1.0"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "predictions_test = model.predict(X_test)\n",
    "prediction_sklearn = sk_model.predict(X_test)\n",
    "\n",
    "test_df = pd.DataFrame({'Metric':['Scratch', 'SkLearn'],\n",
    "                        'Accuracy':[accuracy_score(y_true = y_test, y_pred = predictions_test), accuracy_score(y_true = y_test, y_pred = prediction_sklearn)],\n",
    "                       'Precision':[accuracy_score(y_true = y_test, y_pred = predictions_test), accuracy_score(y_true = y_test, y_pred = prediction_sklearn)],\n",
    "                       'Recall':[accuracy_score(y_true = y_test, y_pred = predictions_test), accuracy_score(y_true = y_test, y_pred = prediction_sklearn)],\n",
    "                       'F1 Score':[accuracy_score(y_true = y_test, y_pred = predictions_test), accuracy_score(y_true = y_test, y_pred = prediction_sklearn)],}).set_index('Metric')\n",
    "test_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 1483,
     "status": "ok",
     "timestamp": 1615702957741,
     "user": {
      "displayName": "Evi Ofekeze",
      "photoUrl": "",
      "userId": "08909702692483848003"
     },
     "user_tz": 420
    },
    "id": "ZSEQbCTLmoN1"
   },
   "source": [
    "### **Appendix : Mathematical formulation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8BJT412LeU2"
   },
   "source": [
    "Consider as Dataset represented by a feature matrix $\\mathbf{x} = (x_{ij}, ...)$ , and an outcome vector $\\mathbf{y} = \\{0,1\\}$\n",
    "\n",
    "We assume a model of the form\n",
    "\n",
    "$$P(y= 1|x) = \\mathbf{w}^T \\mathbf{x} + b$$\n",
    "where $w = weights$ vector  and b is the bias term.\n",
    "\n",
    "\n",
    "We optimise the maximum likehood function for the condtional probability of $y$ given $x$ and $\\theta$\n",
    "\n",
    "\\begin{align}\n",
    "\\text{log} L(\\theta) = \\sum_{i=1}^{m} y_i \\cdot \\text{log} h_{\\theta}(\\mathbf x_i) + (1 - y_i) \\cdot \\text{log}(1 -  h_{\\theta}((\\mathbf x_i)) - \\alpha \\mathbf \\theta^T \\mathbf \\theta\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "The gradient ascent approach gives us the following update rule\n",
    "\n",
    "$$\\theta^{(t)} \\longrightarrow \\theta^{(t-1)} + \\eta \\nabla log L(\\theta) |_{\\theta=\\theta^{(t-q)}}$$\n",
    "\n",
    "now computing $\\nabla log L(\\theta)$:\n",
    "\n",
    "we have \n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_{}\\text{log}(\\theta) &= \\nabla_{\\theta} \\sum^{m}_{i=1} y^{(i)} \\cdot  \\text{log} h_{\\theta}(x^{(i)}) + (1-y^{(i)}) \\cdot \\text{log}(1-h_{\\theta}x^{(i)})\\\\\n",
    "&= \\nabla_{\\theta} \\sum^{m}_{i=1} \\frac{y^{(i)}}{h_{\\theta}(x)}y^{(i)} \\cdot \\nabla_{\\theta} h_{\\theta}(x^{(i)}) + \\frac{(1-y^{(i)})}{1-h_{\\theta}(x^{(i)})} \\cdot \\nabla_{\\theta}(1-h_{\\theta}x^{(i)}) \\tag{1}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkbukWuDY2ry"
   },
   "source": [
    "Evaluting $\\nabla_{\\theta} h_{\\theta}(x^{(i)})$ where $\\theta$ \n",
    "\n",
    "we have\n",
    "\n",
    "$$ h_{\\theta}(\\mathbf x_i) = \\frac{1}{1 + e^{\\mathbf w \\mathbf x + b}}$$\n",
    "\n",
    "Its derivative with respect to $\\theta$ gives us\n",
    "\n",
    "\\begin{align}\n",
    "&=\\frac{-1}{(1 + e^{-(\\textbf{wx} + b)})^2} \\cdot e^{(\\textbf{wx} + b)} \\cdot -\\textbf{x}\\\\\n",
    "&= \\textbf{x} \\cdot \\frac{1}{1 + e^{-(\\textbf{wx} + b)}} \\cdot \\frac{{e^{-(\\textbf{wx} + b)}}}{{1 + e^{-(\\textbf{wx} + b)}}}\\\\\n",
    "&= \\textbf{x} \\cdot \\frac{1}{1 + e^{-(\\textbf{wx} + b)}} \\cdot \\bigg(1- \\frac{1}{{1 + e^{-(\\textbf{wx} + b)}}} \\bigg)\\\\\n",
    "&= \\mathbf x_i \\cdot h_{\\theta}(\\mathbf x_i) \\cdot ( 1 - h_{\\theta}(\\mathbf x_i))\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Since \n",
    "$$ h_{\\theta}(\\mathbf x_i) = \\frac{1}{1 + e^{\\mathbf w \\mathbf x + b}}$$\n",
    "\n",
    "\n",
    "inputing  the value of $\\nabla_{\\theta} h_{\\theta}(x^{(i)})$ in equation 1 and for simplicity represent instances of $x^i$ anf $y^i$ as  $x$ and $y$ and simply ask the obejective to penalise the $\\textbf{L2}$ norm we have \n",
    "\n",
    "\\begin{align} \n",
    "\\nabla_{\\theta} log L(\\theta) &=  \\sum_{i=1}^{m}  \\frac{y_i}{h_{\\theta}(\\mathbf x_i)} \\mathbf x_i \\cdot h_{\\theta}(\\mathbf x_i) ( 1 - h_{\\theta}(\\mathbf x_i))(\\mathbf x_i)  + \\frac{(1 - y_i)}{(1 -  h_{\\theta}) (- \\mathbf x_i} \\cdot h_{\\theta}(\\mathbf x_i) \\cdot ( 1 - h_{\\theta}(\\mathbf x_i)) - \\alpha\\mathbf \\theta \\\\\n",
    "&= \\sum_{i=1}^{m} y  \\cdot  \\textbf x(1-h_{\\theta}(x)) + (y-1) \\textbf x \\cdot \\dot h_{\\theta}(\\textbf x)\\\\\n",
    "&= \\sum_{i=1}^{m} \\textbf x \\cdot y (1-h_{\\theta}(x)) + (y-1) \\dot h_{\\theta}(\\textbf x)\\\\\n",
    "& = \\sum_{i=1}^m \\mathbf x \\cdot(y - h_{\\theta}(\\mathbf x)) - \\alpha\\mathbf \\theta\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "giving us an update rule of \n",
    "\n",
    "$$ \\theta_{i+1} = \\theta_i + \\sum_{i=1}^m \\mathbf x_i \\cdot(y - h_{\\theta}(\\mathbf x_i)) - \\alpha\\mathbf \\theta$$\n",
    "\n",
    "Thus the gradient ascent for the weigths and the bias can be written as \n",
    "\n",
    "$$ w_{i+1} \\longrightarrow w_i + \\sum_{i=1}^m \\mathbf x_i \\cdot(y - h_{\\theta}(\\mathbf x_i)) - \\alpha\\mathbf \\theta \\tag{3}$$\n",
    "\n",
    "$$ b_{i+1} \\longrightarrow b_i + \\sum_{i=1}^m \\mathbf x_i \\cdot(y - h_{\\theta}(\\mathbf x_i)) \\tag{4}$$"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PS2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
